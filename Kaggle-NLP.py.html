#!/usr/bin/env python
# coding: utf-8

# In[48]:


import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords


# In[49]:


data = pd.read_csv("/Users/vanichikaraddi/documents/kaggle/Covid-NLP/Corona_NLP_train.csv",encoding='latin1')
data.head()


# In[50]:


df = pd.DataFrame(data)


# In[51]:


print(df['Sentiment'].unique())


# In[52]:


df.head()


# In[ ]:





# In[53]:


df['Sentiment'].value_counts().plot.pie()


# In[54]:


df['Sentiment'].replace(['Extremely Negative', 'Extremely Positive'], ['Negative', 'Positive'], inplace=True)
df['Sentiment'].value_counts().plot.pie()


# In[55]:


df.info()


# In[56]:


reg = re.compile("(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)")
tweet = []
for i in df["OriginalTweet"]:
    tweet.append(reg.sub(" ", i))
df = pd.concat([df, pd.DataFrame(tweet, columns=["CleanedTweet"])], axis=1, sort=False)


# In[57]:


df.head(10)


# In[58]:


from sklearn.feature_extraction.text import TfidfVectorizer
stop_words = set(stopwords.words('english'))     # make a set of stopwords
vectoriser = TfidfVectorizer(stop_words=None)


# In[59]:


X_train = vectoriser.fit_transform(df["CleanedTweet"])
# Encoding the classes in numerical values
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y_train = encoder.fit_transform(df['Sentiment'])
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(X_train, y_train)


# In[60]:


test_data = pd.read_csv("/Users/vanichikaraddi/documents/kaggle/Covid-NLP/Corona_NLP_test.csv.xls",encoding='latin1')
test_df = pd.DataFrame(test_data)
test_df.head()


# In[61]:


test_df['Sentiment'].replace(['Extremely Negative', 'Extremely Positive'], ['Negative', 'Positive'], inplace=True)
test_df['Sentiment'].value_counts().plot.pie()


# In[62]:


reg1 = re.compile("(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)")
tweet = []
for i in test_df["OriginalTweet"]:
    tweet.append(reg1.sub(" ", i))
test_df = pd.concat([test_df, pd.DataFrame(tweet, columns=["CleanedTweet"])], axis=1, sort=False)
test_df.head()


# In[63]:


X_test = vectoriser.transform(test_df["CleanedTweet"])
y_test = encoder.transform(test_df["Sentiment"])
# Prediction
y_pred = classifier.predict(X_test)
pred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
pred_df.head()


# In[64]:


from sklearn import metrics
# Generate the roc curve using scikit-learn.
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)
plt.plot(fpr, tpr)
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.show()
# Measure the area under the curve. The closer to 1, the "better" the predictions.
print("AUC of the predictions: {0}".format(metrics.auc(fpr, tpr)))


# In[ ]:




